# Webscraping-and-automation

This repository contains webscraping and automation projects.

## Content

1. **[AOPA](https://github.com/lb930/Webscraping-and-automation/tree/master/AOPA)**: This project uses Selenium to scrape all airports and their categories (private, military etc) from [AOPA](https://www.aopa.org/). Each airport name is then used to obtain further details using the AOPA API.

2. **[Decibel](https://github.com/lb930/Webscraping-and-automation/tree/master/Decibel)**: [Decibel](https://decibel.com/heatmaps/) visualises on-page user behaviour, but doesn't provide an option to download screenshots on a daily basis. This script logs into the account, selects a time range, creates a screenshot and downloads it.

3. **[Ecco Verde](https://github.com/lb930/Webscraping-and-automation/tree/master/Ecco%20Verde)**: This project uses Scrapy to obtain all products from a specific manufacturer from the Ecco Verde website. It then extracts all customer reviews.

4. **[FreeAgent](https://github.com/lb930/Webscraping-and-automation/tree/master/FreeAgent)**: Wrote a script which automatically filled out my timesheets for me.

5. **[Ozark](https://github.com/lb930/Webscraping-and-automation/tree/master/Ozark)**: This script collects information on the TV series Ozark. The output contains a list of episodes, their release dates, directors and synopses.

6. **[Signature Flights](https://github.com/lb930/Webscraping-and-automation/tree/master/Signature%20Flights)**: Scrapes all US airports and locations from [Signature](https://www.signatureflight.com/locations?region=us).

7. **[Tableau Public Profiles](https://github.com/lb930/Webscraping-and-automation/tree/master/Tableau%20Public%20Profiles)**: Scrapes each viz, its views and the number of favourites it has received from a given Tableau Public profile.

8. **[Travel Advice](https://github.com/lb930/Webscraping-and-automation/tree/master/Travel%20Advice)**: This project scrapes the [UK foreign travel advice website](https://www.gov.uk/foreign-travel-advice). I started by scraping the URLs for each country and used those to scrape the summary website. 
